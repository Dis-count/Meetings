参考书：

常规优化算法：

算法思想：穷举法,分治法，局部搜索，单纯形法，贪婪算法，动态规划，分支定界，
现代算法基本是启发式优化方法：模拟退火，禁忌搜索，演化算法，遗传算法，神经网络

他们或者能够保证找到全局解但在求解一些典型的实际问题时代价太大，或者容易陷入局部最优。由于很难加速一个
能保证找到最优解的算法，即对于大多数实际问题很难 找到多项式算法，因为他们大多是NP难问题，那么剩下的
选择就是设计出能够跳离局部最优的算法

单纯形法 1947 -- 椭球算法 1979 --  内点法 1984

逻辑线，时间线

凸优化 -- LP,二次规划QP,二次约束的二次规划QCCP，半正定规划SDP 几何规划
二阶锥规划SOCP 属于 linear conic programming
copositive programming

优化问题到方法：定义是什么，有哪些定理保证了可以这么做，为什么要使用这个方法，即这个方法的优点。

比如对于凸优化，什么是convex 进而什么是凸优化，有哪些性质比如局部最优解就是全局最优解
要求目标函数是凸函数，变量所属集合是凸集合的优化问题。或者目标函数是凸函数，变量的约束函数是凸函数（不等式约束时），或者是仿射函数（等式约束时）。

LP:
线性目标，线性约束，向量变元为非负实向量。
SDP： 线性目标，线性约束，对称矩阵变元且为半定实矩阵。

SDP可视为LP的推广，LP的向量分量不等式被矩阵不等式代替。根据半定矩阵的定义知，SDP也可视为一个线性约束的关于变量的无限集的LP，解LP的原始对偶内点法可以推广到SDP。
LP的可行域为有限个顶点的凸多面体，SDP的可行域为一个曲面体。

原始对偶内点法

数值优化  --迭代收敛条件

KKT 条件


线性搜索法： 步长  梯度  梯度是一个很重要的概念，例如在BP算法中权重的更改，梯度的方向指明了误差扩大的方向，因此在更新权重的时候需要对其取反，从而减小权重引起的误差。
最速下降法
全局收敛性关注的是收敛性，也就是说，给定任何的初始点，算法都能收敛到一个驻点。但是局部收敛性更多的关注的是收敛速度。也就是说初始点不能任意选取。
一阶算法：
梯度下降：
共轭梯度法
拟牛顿法
梯度投影法：
蒙特卡洛选初始点进行凸优化得到局部最优解
邻域搜索，爬山算法
矩阵求导
ADMM
SGD
回溯法
列生成 行生成

拉格朗日松弛

对偶法：任意带约束的非凸连续优化问题，其对偶问题作为原问题解的一个lower bound，一定是凸的

凸松弛法：带整数变量的优化问题，松弛之后变成凸优化问题（所以原问题其实是凸优化问题+整数变量）

随机梯度下降法
惩罚函数法：处理约束的常用方法(等式约束)
乘子法
随机优化
两阶段线性规划
鲁棒优化
相位恢复


问题的性质可以促进你想到相应的算法，从而可以有选择的余地。


机器学习优化对象 是模型的参数，数据作为输入，目标函数一般称为损失函数，因为是衡量预测值与真实值的差距 也就是更新和优化这些内参(W，b)
非监督 监督 强化 深度

深度学习中 梯度消失，爆炸
深度卷积网络 反向传播


非凸算法：我们能真正一般化地解决非凸优化问题，那肯定是要对一般的混合整数（线性）规划（MILP, mixed integer linear programming）要有好的办法求解才算。因为任意一个非凸优化问题，都可以用很多的分段线性函数逼近，最后变成一个MILP。


脚本：

总结自己遇到过的坑，对课题不感兴趣导致没有动力去探究，其实是没有看到背后的优化问题和算法。因为很难从现有的文章中得到一些基本的优化问题和算法，但这部分又是必需的。


如何判断该问题要用什么方法，可以用什么方法，要对问题进行分类

从凸优化出发，这一大类问题如何解决

从合作博弈开始讲起 组合优化问题
