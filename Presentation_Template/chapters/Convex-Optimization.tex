\section{General Optimization problem}

    \frame{\sectionpage}

    \begin{frame}{Optimization problem}
      \begin{itemize}
        \item Linear Optimization
        \item Non-Linear Optimization
      \end{itemize}

      \begin{itemize}
        \item Convex Optimization
        \item Non-Convex Optimization
      \end{itemize}
    \end{frame}

    \begin{frame}{Convex Optimization}
      \begin{itemize}
        \item \textcolor{green}{Convex set}:
        A set S is convex if for all members $ x,y\in S $ and all $ \theta \in [0,1] $, we have that $ \theta x+(1-\theta )y\in S $.
        \item \textcolor{green}{Convex function}:
        For all $\theta \in [0,1] $ and all $ x,y $ in $S$, the following condition holds: $f(\theta x+(1-\theta )y)\leq \theta f(x)+(1-\theta )f(y) $.

        \item \textcolor{green}{Convex optimization}:
      \end{itemize}
      \begin{equation*}
        \begin{align}
        \min &\quad f(x) \\
        \text{s.t.} &\quad g_i(x) \leq 0, i=1,2,...,m \\
          &\quad h_j(x) = 0, j = 1,2,...,n
        \end{align}
      \end{equation*}

      \quad where $ \mathbf {x} \in \mathbb {R} ^{n} $ is the optimization variable, the function $ f:{\mathcal {D}}\subseteq \mathbb {R} ^{n}\to \mathbb {R} $ is convex, $ g_{i}:\mathbb {R} ^{n}\to \mathbb {R} $, $ i=1,\ldots ,m $, are convex, and $ h_{i}:\mathbb {R} ^{n}\to \mathbb {R} $, $ i=1,\ldots ,p $, are affine.
    \end{frame}

    \begin{frame}{Standard Problems}
      \vspace{-2pt}
      \begin{itemize}
        \item \textcolor{yellow}{Linear Programming(LP)}
        \[
        \begin{aligned}
        \min &\quad c^Tx+d \\
        s.t. &\quad G(x) \preceq h \\
            &\quad A(x) = b
        \end{aligned}
        \]
        \item \textcolor{yellow}{Quadratic Programming(QP)}
        \[
        \begin{aligned}
        \min &\quad \frac{1}{2}x^TPx+c^Tx+d \\
        s.t. &\quad G(x) \preceq h \\
             &\quad A(x) = b
        \end{aligned}
        \]
        \item \textcolor{yellow}{Semidefinite Programming(SDP)}
        \[
        \begin{aligned}
        \min &\quad tr(CX) \\
        s.t. &\quad tr(A_iX)=b_i, i=1,2,....p \\
             &\quad X \succeq 0
        \end{aligned}
        \]
      \end{itemize}
    \end{frame}

    \begin{frame}{Other problems}
      \Large
      \begin{itemize}
        \item Least squares
        \item Support Vector Machine(SVM)
        \item Quadratically Contrained Quadratic program(QCQP)
        \item Second-Order Cone Program(SOCP)
        \item Geometric Programming(GP)
        \item Conic Optimization
      \end{itemize}
    \end{frame}

    \begin{frame}{Property}
      \begin{itemize}
        \item Local = globle
        \item Non $\to$ Convex
        \item Many method to solve and wide application.
      \end{itemize}
    \end{frame}

    \begin{frame}{Methods}
      \begin{itemize}
        \item Gradient
        Describe the basic iteration process.$x_{k+1}=x_k+\alpha_k d_k$
        $\triangledown f(x_k)^Td_k < 0$
        $f(x_{k+1}) = f(x_k+\alpha_k d_k) < f(x_k)$
        \item Sub-gradient(for can't derivative)
        \item Interior point
      \end{itemize}
    \end{frame}

    \begin{frame}{Gradient}
      Two essential elements: step and gradient. All kinds of methods are based on the fundamental.
      \begin{itemize}
        \item Random
        \item Batch
        \item ......
      \end{itemize}
      convergence condition
    \end{frame}

\begin{frame}{KKT}
  \begin{itemize}
    \item Interior Methods
    \item Penelty
    \item Dual
    \item Lagrange
    \item Augmented Lagrangian
    \item Alternating Direction Method of Multipliers(ADMM)
  \end{itemize}
\end{frame}

\begin{frame}{Non-Convex}
  \item convert to Convex
  \item heuristics
  The important thing is how to jump out of the local optimization.
  Change the parameter by experience.
\end{frame}

\begin{frame}{Random optimization}
  \item Methods
  \item Algorithmic idea
\end{frame}
