走过了很多弯路，今天来总结一下。刚入学的时候，啥也不会，就给老板建议，可以让师兄们讲讲他们做的工作，也好让我有个入门，没想到到头来还是要自己做这样的工作，算是给师弟师妹们做点贡献吧。
我希望用一种提问互动的形式来进行展示。
最开始接触的是合作博弈，也不知道啥是合作博弈，不知道和线性规划啥关系，没什么兴趣，因为一直比较喜欢算法之类的，上过运筹学觉得里面的问题和解法都比较有意思。我希望把我走过的一些弯路都讲给你们，希望你们对自己的工作都感兴趣，然后提高学习科研的热情。
其实当时没有动力和兴趣去研究课题主要是没有看到背后的优化问题和算法。因为很难从现有的文章中得到一些基本的优化问题和算法，但这部分又是必需的。所以呢，今天来大致讲一下优化里面都有哪些方法和应用。

最初大家接触的优化 基本都是如何求解一个函数的最优值，直接求导 或者是对于一般的线性规划都是画出可行域，也就是所谓的图解法，这个我们高中就学过。
我想以一个简单的问题作为起始点，也就是背包问题。这样说一般是0-1背包问题。
背包问题就是将物品装进背包，每个背包有自己的容量和价值，如何选择背包使得在不超过背包容量的前提下装进背包物品的价值得到最大。 我们可以将这个问题建模为一个整数规划模型。
有哪些解法呢？
首先这是一个NPC问题，什么是NPC问题我们后面再说。也就是这个问题是不存在一个多项式算法得到最优解的。如果规模小，我们可以用枚举法，规模大的话存在伪多项式时间的动态规划算法。

<!--
P是否等于NP是计算复杂度理论里面最著名的未解决的问题之一，一个NP完全问题，如果能找到解决它的多项式时间算法，那么就说明了P=NP。

如今0-1背包问题已经被证明是NP完全问题，而它却有着一个动态规划解法，该解法有着O(n*W)的时间复杂度，其中n是物品的个数，W是背包限制的最大负重。所以时间复杂度对输入n，W来说是多项式时间的，所以说明了NP=P！是不是哪里出错了呢？

其实多项式时间是相对于输入规模来说的，输入规模最直观的理解就是输入到该算法的数据占了多少比特内存。0-1背包的输入有n个物品的价值，n个物品的重量，还有背包的最大负重W。如今假设W占用的比特数为L（也就是说背包的最大负重的输入规模是L），那么log(W)=L，所以O(n*W)=O(n*2^L),由此看到，该算法的时间复杂度对于输入规模L来说是指数级别的，随着输入规模L的增加，运算时间会迅速增长。

实际上，人们把这种动态规划的算法称为伪多项式时间算法（pseudo-polynomial time algorithm），这种算法不能真正意义上实现多项式时间内解决问题。 -->
那么动态规划呢，它是一种算法思想，不是具体的算法。
理解动态规划为什么大大会减少计算量非常关键，在某些问题中它的计算复杂度为什么不是指数的，这个是灵活运用的关键。

首先是最优子结构的问题，即他的子问题最优也是可以通过这样的方法求解出来的。比较简单的例子是斐波拉契数列，比如求第n个数，这样的方法对于求解任何小于n的数都是适用的。同时数列中存在一个关系式，这个是我们能够利用动态规划的关键，这样就会提到状态压缩的概念。状态压缩中找到状态的定义是使用动态规划解决问题的关键。如何提取和定义问题的状态能够帮助我们快速建立起关系式，从而使用动态规划求解。在数列中的状态就为n位数，在背包中状态就为所装物品的数量以及背包的容量。
除了在背包问题中有比较经典的应用，另一个经典的应用是最短路问题，它的模型可以写出如下：
尽管最短路问题中有 Dijkstra(正权) SPFA(负权) Ford 算法，但我们主要看动态规划如何求解，如何定义状态呢？

最短路问题   ob:最短路的一部分也是最短路，反证法即可
            穷举可以，但当是指数的时候就不可以
状态status  -- 节点   f_阶段(状态)
阶段stage   -- 经过的边

最短路问题中可以证明任何最短路的子路径相对于子问题始，终点的路径也是最短的。对应到最优子结构的性质。这样定义了状态为阶段以及步数之后，这个最优阶段序列的任何子序列本身一定是相对于子序列的初始和结束状态的最优阶段序列。这样便可以利用动态规划进行求解。
但是在不同问题中动态规划得到的解不是最优解，原因在于他不满足上面提到的最优结构，因此要注意动态规划求解的条件。
多阶段决策过程，每步求解的问题是后面阶段求解问题的子问题，每步决策将依赖于以前步骤的决策结果。

依次 考虑从 C 到 T 的最短距离。
考虑从 B 到 C 的最短距离
考虑从 A  到 B 的最短距离
考虑从 S 到 A 的最短距离
每次都是最短距离。

在整个过程中，我们把 我们的目标问题转化成了一个个的子问题，在子问题 求 最小值，最后解决了这个问题。

总结因为求解过程是一个多阶段的过程，每一步处理一个子问题，可用于求解组合优化问题。适用条件满足最优子结构的性质。


回头看该问题的整数规划模型，可以看出对应的系数矩阵是 totally unimodular matrix, 该矩阵的特点是
如果 $A \in R^{m\times n}$是整数矩阵，r =rank(A)
而且A的所有非零r×r子式等于 1 或-1，则称A为幺模矩阵, 如果A是幺模矩阵，而且还有其各阶子式均等于0，1或-1，则称A为全幺模矩阵.
显然，全幺模矩阵的所有元素均为0，1或-1。
Totally Unimodular 三个条件
1. 元素均为 +1 -1 0
2. 每一列只有不超过两个的非零元素
3. 行可以分为两组，对于每一列这两组数相加相等。

当然这里面会牵扯到非常多的比较高深的数学知识，但对我们来说重要的是，如果一个整数规划它的矩阵是 totally unimodular, 那么我们就可以利用线性规划来求解它，这是一个非常好的性质。虽然这个性质是可以从 totally unimodular出发得到的，但我们可以从另一个方面来看这里面比较有趣的性质。
回到线性规划的单纯形法解，来到最后一步， 得到解 x* = (B-1b,0)   B-1 = B*/det(B)
矩阵的逆怎么求？克莱姆法则，对于 matrix 的逆必然是正负一，因而解是整数。

那有哪些问题是全单模的呢？
指派问题，最短路，最大流问题

好了这部分的整数规划问题可以转化为线性规划进行求解，那么线性规划大家都很熟悉，尽管单纯形法本身不是多项式可解（在最坏的情况下）但掌握单纯形法的具体流程还是比较重要的，比如我们上述讲整数规划和线性规划关系的时候就用到了，以及一些在单纯形法基础上进行改进的算法，比如二阶段线性规划，掌握检验数，基变量非基变量，基本可行解对于理解基本的列生成算法，割平面方法也是有帮助的。进基出基过程：
单纯形法中看迭代过程  解为 x
可以说，设边的方向为λ，我们沿着边走的距离是θ，那么，我们走的就是 x' = x – θλ
那么λ是什么呢？其实就是选择一个非基的向量（进基过程），可以理解为梯度方向。
θ就是步长，步长越大越好（出基过程），最大只能走到可行域的顶点。
https://www.hrwhisper.me/introduction-to-simplex-algorithm/
为什么要讲这个呢，因为一直觉得单纯形法很另类，觉得线性规划的解法和一般的优化问题不太一样，但实际还是可以看出是一个梯度下降的过程。
提到了一般的优化问题，那么来看一下针对一般的优化问题都有哪些优化方法。
接着讲分类
分别有精确算法，启发式算法和近似算法。精确算法一般是
分支算法一大类，分支定界最基本的利用定界来减少计算量的方法，分支割可以理解为不断生成割平面添加约束，分支price是分支定界和列生成的结合，主问题子问题分清楚，难点一般在子问题pricing problem(seperation probelm 一般是行生成中的子问题)。
每次计算过程中添加列加快计算过程，这个大家都听了比较多了，了解基本过程之后可以试试简单的例子，做一下总结。这样遇到问题可能需要这个方法的时候就可以很快利用上。

注意分支定界虽然是精确算法 但由于是一个迭代算法，存在收敛的问题，因而通常设置gap 提供终止条件。

约束编程问题 或是约束满足问题
我的理解是 即不需要找到一个最优解，而是要找到满足约束的解  著名的八皇后问题  图着色问题
这种一般是能够建立起满足条件的约束就可以得到问题的解，因而认为一种精确的算法，当然这部分如果是整数规划可以结合割平面，分支定界，动态规划进行求解。

动态规划前面讲了是一种算法思想，掌握它的优化条件，即什么问题适合利用该方法能够得到最优解。

接着是近似方法，近似方法分为启发式算法和近似算法。近似算法是一大类，大家平常见到的 1+\epsilon O(\epsilon^-1) 这类有近似比的算法表述就是近似算法，它的特点是能够表示出算法得到的解与最优解之间的误差，需要的分析知识较多，证明方法多样非常灵活，要求比较高。在调度问题，设施选址问题中出现的比较多。

然后是所谓的现代优化算法，之前我还是比较觉得这类算法比较高大上，但现在不这么想了，当然主要是我自己这么想。主要问题在于算法得到的结果没有办法证明与最优解的关系，也就是没法判断该算法的优劣。只能通过数值实验去证明，适用性和可迁移性不强，需要去试，对一个问题好就是好，但对另一个问题就不好。但我并不是说启发式不重要，在实际生活中，基本不存在能够利用精确算法求解的问题，大多都是利用近似方法求解的，在这里面精确算法和启发式方法都发挥重要的作用。

基本所有的启发式都是为了跳出局部最优解。
分为元启发式和具体问题的启发式，具体问题就是上面说的得到数值结果好就好，也说不出来具体原因。
 “元启发式算法”就是：用来构造启发式算法的一些基础方法。元是一种算法框架，
“启发式算法”就是：利用元启发式算法，结合被求解问题的特征，设计出来的面向特定问题的算法。

基于单个解的元启发式
模拟退火（Simulated Annealing Algorithm）
禁忌搜索（Tabu Search）
变邻域搜索（Variable Neighborhood Search）
自适应大规模领域搜索（Adaptive Large Neighborhood Search）


基于群体（Population-based heuristics）的启发式方法
1、遗传算法（Genetic Algorithm）
2、蚁群算法
3、粒子群算法

启发式算法是相对“最优算法”而言的，其目标是在某种启发原则的引导下搜寻解（这种解一般是局部最优，但可以很大程上接近最优）；贪心算法的核心——贪心准则就是一种启发原则。

说完了不同的算法区别，我们再来看看有哪些经典的缩写问题 像是TSP，VRP，FLP， Production Scheduling Problem
这些问题呢都是组合优化问题，基本上组合优化问题可以分类为 集合划分问题Set Partition 排序问题 选址问题。注意TSP也是一种排序问题。存在大量的离散变量组合的结果，组合数大家都知道是指数增长的，可行解的数量是呈指数增长。要选出一个最优解就变得极为困难。因而找出一种可以高效率解决组合优化问题的算法是大家一直追求的目标。

说到组合优化不得不提到的概念就是NP-难，首先需要了解 P 和 NP的概念，
P类问题：所有可以在多项式时间内求解的判定问题构成P类问题。
判定问题或者说是决策问题：判断是否有一种能够解决某一类问题的算法。
简单来说也就是 是否存在，答案是Yes or No.

NP类问题：所有的非确定性多项式时间可解的判定问题构成NP类问题。
非确定性算法：非确定性算法将问题分解成猜测和验证两个阶段。算法的猜测阶段是非确定性的，算法的验证阶段是确定性的，它验证猜测阶段给出解的正确性。
设算法A是解一个判定问题Q的非确定性算法，如果A的验证阶段能在多项式时间内完成，则称A是一个多项式时间非确定性算法。有些计算问题是确定性的，例如加减乘除，只要按照公式推导，按部就班一步步来，就可以得到结果。但是，有些问题是无法按部就班直接地计算出来。比如，找大质数的问题。有没有一个公式能推出下一个质数是多少呢？这种问题的答案，是无法直接计算得到的，只能通过间接的“猜算”来得到结果。这也就是非确定性问题。而这些问题的通常有个算法，它不能直接告诉你答案是什么，但可以告诉你，某个可能的结果是正确的答案还是错误的。这个可以告诉你“猜算”的答案正确与否的算法，假如可以在多项式（polynomial）时间内算出来，就叫做多项式非确定性问题。

简单来说就是 NP 验证是对的 是多项式容易的。但是算法是不确定的，即求解较难。
比如说 判断给定 一个数N的分解 是否等于两个质数的乘积 K * L 很容易判断是否。但是让你求解 N 是否为质数就是一个非确定算法。


NPC问题：所有的NP问题都可以在多项式时间内转化为 P \in NP，则P是最难的问题。
        称 P \in NPC.
这些问题被称为NP-完全问题(NPC问题)。  也有说NPH的，注意NPC 可以是多项式时间内无法验证正确与否的。

NP-hard 是指优化问题对应的 决策问题是 NPC 完备的。

<!-- 最短路问题 O(m^2)   强多项式时间
解 Ax=b   多项式时间  高斯消元法  n次转轴 n^2 计算  n^3
指派问题  O(n^4)
线性规划  内点法 -->
例子：
刚才说到的质数问题：
如果某人告诉你，数13,717,421可以写成两个较小的数的乘积，你可能不知道是否应该相信他，但是如果他告诉你他可以因式分解为3607乘上3803

又比如我和师弟去美广吃饭，师弟不知道我在哪个位置，但是我告诉师弟我经常坐在电视机下面，a他只需要判断电视机下是不是我就可以了，而不需要绕食堂一周。

<!-- 在一个周六的晚上，你参加了一个盛大的晚会。由于感到局促不安，你想知道这一大厅中是否有你已经认识的人。你的主人向你提议说，你一定认识那位正在甜点盘附近角落的女士罗丝。不费一秒钟，你就能向那里扫视，并且发现你的主人是正确的。然而，如果没有这样的暗示，你就必须环顾整个大厅，一个个地审视每一个人，看是否有你认识的人。
生成问题的一个解通常比验证一个给定的解时间花费要多得多。这是这种一般现象的一个例子。与此类似的是，如果某人告诉你，数13,717,421可以写成两个较小的数的乘积，你可能不知道是否应该相信他，但是如果他告诉你他可以因式分解为3607乘上3803，那么你就可以用一个袖珍计算器容易验证这是对的。人们发现，所有的完全多项式非确定性问题，都可以转换为一类叫做满足性问题的逻辑运算问题。既然这类问题的所有可能答案，都可以在多项式时间内计算，人们于是就猜想，是否这类问题，存在一个确定性算法，可以在多项式时间内，直接算出或是搜寻出正确的答案呢？这就是著名的NP=P？的猜想。 不管我们编写程序是否灵巧，判定一个答案是可以很快利用内部知识来验证，还是没有这样的提示而需要花费大量时间来求解，被看作逻辑和计算机科学中最突出的问题之一。它是斯蒂文·考克于1971年陈述的。 -->

那么像这样的组合问题有哪些解法呢？分支定界，割平面，行生成，列生成，彼此互相组合衍生出来的更为高效的算法，需要具体问题具体分析，但需要掌握最基本的原理。

一般的优化算法，对于凸优化以及非凸优化的特点，凸优化因为可以求得最优解因而在应用上非常重要，最基本的方法是梯度算法，

线性搜索法
前面我们提到了单纯形法中旋转操作可以通过一个迭代过程表示出来，这是一个标准的迭代过程。梯度算法重要的两个概念就是 步长和梯度，有一系列分析的条件去讨论这里面的收敛性，这也是前面提到的一些近似算法能够分析出近似比的原因。但我们不用掌握这里面具体的收敛特性所需要满足的条件，其实是因为我不会。而启发式算法中也会有类似的梯度算法，它们只是拿来用也是不分析收敛具体速度的。

全局收敛性关注的是收敛性，也就是说，给定任何的初始点，算法都能收敛到一个驻点。但是局部收敛性更多的关注的是收敛速度。也就是说初始点不能任意选取。

凸优化问题有一个重要的特性，局部等于全局


比如对于凸优化，什么是convex 进而什么是凸优化，有哪些性质比如局部最优解就是全局最优解
要求目标函数是凸函数，变量所属集合是凸集合的优化问题。或者目标函数是凸函数，变量的约束函数是凸函数（不等式约束时），或者是仿射函数（等式约束时）。


以及大名鼎鼎的KKT算法（由KKT这部分可以衍生出二次规划），内点法以及拉格朗日乘子法以及由此产生的对偶算法，对偶算法的强对偶性质。
由此衍生出来的各种梯度算法，批量梯度，随机梯度算法，只要掌握梯度算法的原理即可。以及如果是非凸的函数，可以采用启发式算法，注意启发式算法是一般是没有近似比的，因为作为一种智能算法，有一定的局限性，一般靠经验和调整参数。重点在于能够设计出跳出局部最优解的算法。最后是随机优化有哪些方法。算法思想有哪些，最后要掌握 遇到一个问题用那种方法如何对号入座，这样遇到问题就会有较多的思路，多尝试不同的方法。


具体实践过程中，
